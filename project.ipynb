{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8557667-168d-4acb-92a4-c1bb47d9dd5d",
   "metadata": {},
   "source": [
    "# Classifying Suspicious Firms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8795a3-3126-4d6e-878d-c40a37c8431e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Citation of Data\n",
    "Hooda,Nishtha. (2018). Audit Data. UCI Machine Learning Repository. https://doi.org/10.24432/C5930Q."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e3a5b-38e3-4417-b838-afbd507b1743",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Introduction \n",
    "\n",
    "Having the need to interact with firms on a daily basis, many people in the world have been victims of fraudulent firms. Although auditors may be able to track a few down, not all can be located. Can we use present and historical risk factors to determine if a firm can be labeled as a fraudulent risk or not?\n",
    "\n",
    "The data, stored as a CSV file, contains one year (2015-2016) exhaustive information on firms that is collected by the Auditor Office of India. The aim of this project is to use this data to develop a model which will help auditors by classifying and finding fraudulent firms. A variety of risk factors are examined in order to reach a score of either 1 (fraudulent) or 0 (not fraudulent) for a firm. There are missing values in the dataset that will be corrected by tidying the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ea4fd-b391-440f-82da-e328a13f4b38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preliminary exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a147e7-c36b-4d64-bd78-401d59d2acf1",
   "metadata": {},
   "source": [
    "First, we load in the packages as well as loading in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b07939-24a8-4d81-9af1-0c0b12b170b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set.seed(888)\n",
    "\n",
    "# Loads in the packages\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "\n",
    "# Loads the data into the variable\n",
    "url <- \"https://raw.githubusercontent.com/alexander-zhong/dsci-100-project/main/data/audit_risk.csv\"\n",
    "audit_risk_data <- read_csv(url) \n",
    "\n",
    "head(audit_risk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ec8d9-7b81-4520-a9bf-61000432031d",
   "metadata": {
    "tags": []
   },
   "source": [
    "As we can see from the preview, there is a lot of tidying and cleaning of\n",
    "the data that needs to be done before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4c221-bf47-40ad-a248-45d5aa9784a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and turn the Risk variable into a factor for our classification\n",
    "audit_risk_data <- audit_risk_data |>\n",
    "    mutate(Risk = as_factor(Risk)) |>\n",
    "    mutate(Risk = fct_recode(Risk, \"Fraudulent\" = \"1\", \"Not Fraudulent\" = \"0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37c2ab-241e-4610-bfef-3e6b38efb7f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lets quickly take a look at our data by representing the data with a few \n",
    "tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7fd26-e97d-4dcb-807e-87f4bac612c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see the total number of firms that are considered a risk or not a risk\n",
    "preliminary_table1 <- audit_risk_train |>\n",
    "    group_by(Risk) |>\n",
    "    summarize(Count = n())\n",
    "preliminary_table1\n",
    "\n",
    "\n",
    "# see the average of our predictor variables\n",
    "preliminary_table2 <- audit_risk_train |>\n",
    "    select(CONTROL_RISK, Audit_Risk, Inherent_Risk) |>\n",
    "    map_df(mean, na.rm = TRUE)\n",
    "\n",
    "preliminary_table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d86462-e2f9-41b2-aab5-d7a8bbf1e9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# splitting the data \n",
    "audit_risk_split <- initial_split(audit_risk_data, prop = .7, strata = Risk)\n",
    "audit_risk_train <- training(audit_risk_split)\n",
    "audit_risk_test <- testing(audit_risk_split)\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "################## \n",
    "audit_recipe <- recipe(Risk ~ CONTROL_RISK + Audit_Risk + Inherent_Risk, data = audit_risk_train) |>\n",
    "    step_scale(all_predictors()) |>\n",
    "    step_center(all_predictors()) |>\n",
    "    prep()\n",
    "\n",
    "audit_risk_train_scaled <- bake(audit_recipe, audit_risk_train)\n",
    "\n",
    "audit_risk_train_scaled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preliminary_plot1 <- audit_risk_train |> \n",
    "    ggplot(aes(x = Inherent_Risk, y = Audit_Risk, color = Risk)) +\n",
    "    geom_point(alpha = 0.4) + \n",
    "    labs(x = \"Inherent Risk (Unstandardized)\", y = \"Audit Risk (Unstandardized)\") +\n",
    "    ggtitle(\"Inherent Risk against Audit Risk\") +\n",
    "    scale_x_log10(labels = label_comma()) +\n",
    "    scale_y_log10(labels = label_comma()) + \n",
    "    ggtitle(\"Inherent Risk against Audit Risk (Scaled with Log base of 10)\")\n",
    "preliminary_plot1\n",
    "\n",
    "\n",
    "preliminary_plot2 <- audit_risk_train_scaled |> \n",
    "    ggplot(aes(x = Inherent_Risk, y = Audit_Risk, color = Risk)) +\n",
    "    geom_point(alpha = 0.2) + \n",
    "    labs(x = \"Inherent Risk (Standardized)\", y = \"Audit Risk (Standardized)\") +\n",
    "    ggtitle(\"Standardized Inherent Risk against Audit Risk\")\n",
    "preliminary_plot2\n",
    "\n",
    "preliminary_plot3 <- audit_risk_train_scaled |> \n",
    "    ggplot(aes(x = Inherent_Risk, y = Audit_Risk, color = Risk)) +\n",
    "    geom_point(alpha = 0.4) + \n",
    "    labs(x = \"Inherent Risk (Standardized)\", y = \"Audit Risk (Standardized)\") +\n",
    "    xlim(c(-0.5, 0.5)) +\n",
    "    ylim(c(-0.5, 0.5)) +\n",
    "    ggtitle(\"Zoomed in Inherent Risk against Audit Risk\")\n",
    "preliminary_plot3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80808cd-cd73-40a7-af9a-ca50043e884f",
   "metadata": {},
   "source": [
    "## Methods\n",
    "We will conduct our data analysis by setting 70% of the data as training data and 30% as testing data. We will only use three variables: Inherent Risk, Control Risk, and Audit risk to predict if a firm is classified as a risk. First, we need to scale/standardize these three variables. Then, we will plot six scatter plots in total using training data. Do cross-validation to find the most optimal K value. Finally, we will use testing data to test the accuracy of our prediction and gather some metrics such as precision, recall and accuracy.\n",
    "\n",
    "One way we will visualize the results is by plotting a scatter plot with Inherent on the x-axis and Control Risk on the y-axis (as shown above). We also color the dots to indicate if the firm represented by each dot is classified as risk or not. Then, we can possibly create a plot of the predicted points on top of the training scatter plot and color the points that were wrongly predicted in red while the points that were predicted correctly in green. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d132734-b2f7-41cc-b419-6f85f5c7151a",
   "metadata": {},
   "source": [
    "## Expected outcomes and significance\n",
    "\n",
    "Throughout this project, we expect to find which predictors, and k values are best for our model when training and testing our classifier to identify fraudulent firms. Using the data, we can understand the relationship between risk factors and build a possibly reliable predictive model that has a balance of good accuracy, recall and precision. \n",
    "\n",
    "Our model that is built through this project could be a pioneer for future models that could be used by audit companies around the world to filter through firms faster. This would allow audit offices to develop early countermeasures.\n",
    "\n",
    "Our model could also lead to future development in new risk factors that the data may not have accounted for.\n",
    "\n",
    "Due to the fact that many people around the world still fall victim to fraudulent firms, our model could also act as a tool for the public to easily identify whether a firm is suspicious or not.\n",
    "\n",
    "This leads us to ask, are there any predictors that could increase precision, recall, and accuracy at the same time? How can we make this tool more accessible to everyone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5724119-96d8-4641-82b7-893a64ee8e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
